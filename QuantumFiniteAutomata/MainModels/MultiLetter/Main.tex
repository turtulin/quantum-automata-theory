\subsection{Multi-Letter Quantum Finite Automata}
\label{sec:multi-letter-qfa}

\gls{ml-qfa} are a generalization of traditional \glspl{qfa}, where transitions depend on multiple letters read from the input, rather than a single letter. This allows them to capture more complex patterns in the input string.

\subsubsection{\glsentryfull{ml-qfa}}

\gls{ml-qfa} were introduced to extend the capability of classical and quantum models by applying unitary operations based on the last $k$ letters read rather than just one. This enables them to recognise languages outside the reach of traditional measure-once or measure-many 1QFA models \cite{belovs2007multi}.

\begin{definition}[\gls{ml-qfa}]
A $k$-letter \gls{ml-qfa} is a 5-tuple $A = (Q, Q_{acc}, \ket{\psi_0}, \Sigma, \mu')$, where:
\begin{itemize}
    \item $Q$ is a finite set of states,
    \item $Q_{acc} \subseteq Q$ is the set of accepting states,
    \item $\ket{\psi_0}$ is the initial superposition of states with unit norm,
    \item $\Sigma$ is the input alphabet,
    \item $\mu': (\Sigma \cup \{\Lambda\})^k \to U(\mathbb{C}^n)$ is a function assigning a unitary operator to every $k$-tuple of symbols.
\end{itemize}
\end{definition}

The transition applies the unitary associated with the last $k$ letters read. For input $\omega = x_1 x_2 \dots x_n$, the computation evolves through unitary applications as specified in Eq. (1) and acceptance is determined using a projection operator $P_{acc}$ \cite{qiu2009hierarchy}.

\paragraph{Strings Acceptance}
Acceptance is defined by exact probability, cutpoint (strict or non-strict), or bounded-error depending on how $P_A(\omega) = \| P_{acc} U_\omega \ket{\psi_0} \|^2$ compares to a threshold $\lambda$.

\paragraph{Sets of Languages Accepted}
ML-QFA can recognise a proper superset of \glspl{reg} compared to MO-1QFA and MM-1QFA. Notably, they can recognise the language $(a + b)^*a$ which is not recognizable by MO-1QFA or MM-1QFA \cite{belovs2007multi}.

\paragraph{Closure Properties}
The class of languages recognised by ML-QFA is not closed under union, intersection, or complement, especially under non-strict cutpoint semantics \cite{qiu2009hierarchy}.

\paragraph{Advantages and Limitations}
ML-QFA demonstrate higher computational power with fewer states in certain scenarios. However, equivalence and minimization are complex and computationally hard. For non-strict cutpoints, the emptiness problem is undecidable \cite{qiu2008decidability}.

\paragraph{Comparison}
ML-QFA are more expressive than MO-1QFA and MM-1QFA under the same acceptance criteria, but less so than two-way or general quantum automata with additional memory models \cite{qiu2011multi}.

\paragraph{Additional Topics}
Research is ongoing in determining exact hierarchies, equivalence testing, and applying ML-QFA in quantum protocol verification \cite{lin2012equivalence, qiu2008decidability}.

\begin{example}
The language $(a + b)^*a$ is a canonical example recognised by 2-letter ML-QFA, using a transition function dependent on the last two characters read \cite{belovs2007multi}.
\end{example}

\subsubsection{\glsentryfull{ml-mmqfa}}

\gls{ml-mmqfa} combine the power of measure-many acceptance strategies with multiletter transitions, extending both classical MM-1QFA and ML-QFA. In this model, a measurement is performed after each quantum evolution step, but the evolution itself depends on the last $k$ letters read. This hybrid structure allows \gls{ml-mmqfa} to accept more complex languages than ML-QFA or MM-1QFA alone \cite{lin2012equivalence}.

\begin{definition}[\gls{ml-mmqfa}]
A $k$-letter \gls{ml-mmqfa} is a 7-tuple \[A = (Q, Q_{acc}, Q_{rej}, \ket{\psi_0}, \Sigma, \mu', \mathcal{O}),\] where:
\begin{itemize}
    \item $Q$ is the finite set of states,
    \item $Q_{acc} \subset Q$ is the set of accepting states,
    \item $Q_{rej} \subset Q$ is the set of rejecting states with $Q_{acc} \cap Q_{rej} = \emptyset$,
    \item $\ket{\psi_0}$ is the initial state,
    \item $\Sigma$ is the finite input alphabet,
    \item $\mu': (\Sigma \cup \{\Lambda, \text{\textsterling}, \$\})^k \to U(\mathbb{C}^n)$ assigns a unitary matrix to each $k$-letter word,
    \item $\mathcal{O} = \{P_{acc}, P_{rej}, P_{non}\}$ is a projective measurement partitioning the Hilbert space based on $Q_{acc}$, $Q_{rej}$, and the non-halting subspace.
\end{itemize}
\end{definition}

Computation starts with the end-marked input Â£$x_1x_2 \dots x_n\$$, and proceeds by interleaving unitary evolutions with projective measurements.

\paragraph{Strings Acceptance}
For a word $\omega = x_1 x_2 \dots x_n$, the acceptance probability is:
\[
P_A(\omega) = \sum_{i=0}^{n+1} \left\| P_{acc} U_{x_i} \left( \prod_{j=i-1}^{0} P_{non} U_{x_j} \right) \ket{\psi_0} \right\|^2
\]
A string is accepted if this probability exceeds a cutpoint (for probabilistic acceptance), or is exactly 1 (for exact acceptance). Both strict and non-strict cutpoints are considered \cite{lin2012equivalence}.

\paragraph{Sets of Languages Accepted}
\gls{ml-mmqfa} can recognise languages beyond the regular class. They accept some languages that are not accepted by classical QFA or even standard MM-1QFA, especially under non-strict cutpoint semantics \cite{qiu2009hierarchy, lin2012equivalence}.

\paragraph{Closure Properties}
The set of languages recognised by \gls{ml-mmqfa} is not closed under union, intersection, or complementation for general acceptance modes. Notably, these properties depend heavily on the acceptance criteria used (bounded-error, cutpoint, etc.) \cite{qiu2009hierarchy}.

\paragraph{Advantages and Limitations}
\gls{ml-mmqfa} are more powerful than both ML-QFA and MM-1QFA. However, they inherit the undecidability of the emptiness and equivalence problems for non-strict and strict cutpoint semantics \cite{qiu2008decidability, lin2012equivalence}. Their expressive power comes at the cost of analytical and implementation complexity.

\paragraph{Comparison} 
\gls{ml-mmqfa} strictly subsume ML-QFA under the same input size and acceptance criteria. They are not comparable in power with general QFA models that allow additional memory or two-way movement. Compared to MM-1QFA, \gls{ml-mmqfa} can accept non-stochastic languages \cite{qiu2009hierarchy}.

\paragraph{Additional Topics}
Further work focuses on state complexity, succinctness, and simulation algorithms between different classes. The diagonal sum construction plays a key role in analyzing equivalence and decidability \cite{lin2012equivalence}.

\begin{example}
An \gls{ml-mmqfa} with 2-letter transitions and intermediate measurements can accept the language $(a+b)^*a$ with higher robustness to probabilistic acceptance thresholds than an ML-QFA \cite{belovs2007multi}.
\end{example}


\subsubsection{\glsentryfull{ml-revqfa}}
\gls{ml-revqfa} extend the multiletter framework by imposing reversibility constraints on the quantum evolution. These automata are designed so that each computation step is invertible, maintaining the core principle of reversibility from quantum mechanics and enhancing coherence preservation \cite{belovs2007multi}.

\begin{definition}[\gls{ml-revqfa}]
A $k$-letter \gls{ml-revqfa} is a special case of $k$-letter \gls{ml-qfa} where each unitary transformation $\mu'(\omega)$ satisfies the additional constraint that $\mu'(\omega)^{-1} = \mu'(\omega)^\dagger$ for all $\omega \in (\Sigma \cup \{\Lambda\})^k$, and the set of such transformations forms a group under composition.
The automaton is defined as a 5-tuple $A = (Q, Q_{acc}, \ket{\psi_0}, \Sigma, \mu')$, with $\mu'$ restricted to reversible unitary matrices.
\end{definition}

\paragraph{Strings Acceptance}
Acceptance is defined as in ML-QFA using a projector $P_{acc}$. For an input string $\omega$, the acceptance probability is:
\[
P_A(\omega) = \| P_{acc} U_{\omega} \ket{\psi_0} \|^2
\]
with $U_{\omega}$ computed from the sequence of reversible unitaries associated with the $k$-letter substrings \cite{belovs2007multi}.

\paragraph{Sets of Languages Accepted}
\gls{ml-revqfa} are strictly more limited than general ML-QFA due to their reversibility constraint. They can recognise a subset of \glspl{reg} and do not accept all \glspl{reg} with bounded error, particularly under exact acceptance criteria.

\paragraph{Closure Properties}
The class of languages recognised by \gls{ml-revqfa} is not closed under union or complement. Reversibility limits the computational power of these models in comparison with more general multiletter QFA models \cite{belovs2007multi}.

\paragraph{Advantages and Limitations}
Reversible automata are appealing for quantum computing implementations due to better coherence and energy efficiency. However, their expressiveness is constrained. \gls{ml-revqfa} cannot recognise some simple \glspl{reg} that non-reversible ML-QFA can handle \cite{belovs2007multi}.

\paragraph{Comparison}
\gls{ml-revqfa} are less powerful than both ML-QFA and \gls{ml-mmqfa}. They are closely related to reversible classical automata and group automata. Compared to non-reversible models, they generally require more states or cannot recognise the same languages under equivalent semantics \cite{belovs2007multi}.

\paragraph{Additional Topics}
Future research may explore connections with quantum error correction, fault-tolerant reversible computing, and applications in energy-efficient quantum hardware design.

\begin{example}
It was shown that \gls{ml-revqfa} cannot accept the language $(a+b)^*a$ even when using multiletter transitions, while a general ML-QFA can accept it with bounded error \cite{belovs2007multi}.
\end{example}
