\subsection{Multi-letter Models}
\label{sec:multi-letter-qfa}

Multiletter Quantum Finite Automata (\gls{ml-qfa}) are a generalization of traditional quantum finite automata, where transitions depend on multiple letters read from the input, rather than a single letter. This allows them to capture more complex patterns in the input string.

\subsubsection{\glsentryfull{ml-qfa}}

Multiletter QFA were introduced to extend the capability of classical and quantum models by applying unitary operations based on the last $k$ letters read rather than just one. This enables them to recognize languages outside the reach of traditional measure-once or measure-many 1QFA models \cite{belovs2007multi}.

\paragraph{Formal Definition}
A $k$-letter \gls{ml-qfa} is a 5-tuple $A = (Q, Q_{acc}, \ket{\psi_0}, \Sigma, \mu')$, where:
\begin{itemize}
    \item $Q$ is a finite set of states,
    \item $Q_{acc} \subseteq Q$ is the set of accepting states,
    \item $\ket{\psi_0}$ is the initial superposition of states with unit norm,
    \item $\Sigma$ is the input alphabet,
    \item $\mu': (\Sigma \cup \{\Lambda\})^k \to U(\mathbb{C}^n)$ is a function assigning a unitary operator to every $k$-tuple of symbols.
\end{itemize}
The transition applies the unitary associated with the last $k$ letters read. For input $\omega = x_1 x_2 \dots x_n$, the computation evolves through unitary applications as specified in Eq. (1) and acceptance is determined using a projection operator $P_{acc}$ \cite{qiu2009hierarchy}.

\paragraph{Strings Acceptance}
Acceptance is defined by exact probability, cutpoint (strict or non-strict), or bounded-error depending on how $P_A(\omega) = \| P_{acc} U_\omega \ket{\psi_0} \|^2$ compares to a threshold $\lambda$.

\paragraph{Sets of Languages Accepted}
ML-QFA can recognize a proper superset of regular languages compared to MO-1QFA and MM-1QFA. Notably, they can recognize the language $(a + b)^*a$ which is not recognizable by MO-1QFA or MM-1QFA \cite{belovs2007multi}.

\paragraph{Closure Properties}
The class of languages recognized by ML-QFA is not closed under union, intersection, or complement, especially under non-strict cutpoint semantics \cite{qiu2009hierarchy}.

\paragraph{Advantages and Limitations}
ML-QFA demonstrate higher computational power with fewer states in certain scenarios. However, equivalence and minimization are complex and computationally hard. For non-strict cutpoints, the emptiness problem is undecidable \cite{qiu2008decidability}.

\paragraph{Comparison}
ML-QFA are more expressive than MO-1QFA and MM-1QFA under the same acceptance criteria, but less so than two-way or general quantum automata with additional memory models \cite{qiu2011multi}.

\paragraph{Example}
The language $(a + b)^*a$ is a canonical example recognized by 2-letter ML-QFA, using a transition function dependent on the last two characters read \cite{belovs2007multi}.

\paragraph{Additional Topics}
Research is ongoing in determining exact hierarchies, equivalence testing, and applying ML-QFA in quantum protocol verification \cite{lin2012equivalence, qiu2008decidability}.

\subsubsection{Multiletter Measure-Many Quantum Finite Automata (ML-MMQFA)}

\paragraph{Introduction}
\gls{ml-mmqfa} combine the power of measure-many acceptance strategies with multiletter transitions, extending both classical MM-1QFA and ML-QFA. In this model, a measurement is performed after each quantum evolution step, but the evolution itself depends on the last $k$ letters read. This hybrid structure allows ML-MMQFA to accept more complex languages than ML-QFA or MM-1QFA alone \cite{lin2012equivalence}.

\paragraph{Formal Definition}
A $k$-letter \gls{ml-mmqfa} is a 7-tuple $A = (Q, Q_{acc}, Q_{rej}, \ket{\psi_0}, \Sigma, \mu', \mathcal{O})$, where:
\begin{itemize}
    \item $Q$ is the finite set of states,
    \item $Q_{acc} \subset Q$ is the set of accepting states,
    \item $Q_{rej} \subset Q$ is the set of rejecting states with $Q_{acc} \cap Q_{rej} = \emptyset$,
    \item $\ket{\psi_0}$ is the initial state,
    \item $\Sigma$ is the finite input alphabet,
    \item $\mu': (\Sigma \cup \{\Lambda, \text{\textsterling}, \$\})^k \to U(\mathbb{C}^n)$ assigns a unitary matrix to each $k$-letter word,
    \item $\mathcal{O} = \{P_{acc}, P_{rej}, P_{non}\}$ is a projective measurement partitioning the Hilbert space based on $Q_{acc}$, $Q_{rej}$, and the non-halting subspace.
\end{itemize}
Computation starts with the end-marked input Â£$x_1x_2 \dots x_n\$$, and proceeds by interleaving unitary evolutions with projective measurements.

\paragraph{Strings Acceptance}
For a word $\omega = x_1 x_2 \dots x_n$, the acceptance probability is:
\[
P_A(\omega) = \sum_{i=0}^{n+1} \left\| P_{acc} U_{x_i} \left( \prod_{j=i-1}^{0} P_{non} U_{x_j} \right) \ket{\psi_0} \right\|^2
\]
A string is accepted if this probability exceeds a cutpoint (for probabilistic acceptance), or is exactly 1 (for exact acceptance). Both strict and non-strict cutpoints are considered \cite{lin2012equivalence}.

\paragraph{Sets of Languages Accepted}
ML-MMQFA can recognize languages beyond the regular class. They accept some languages that are not accepted by classical QFA or even standard MM-1QFA, especially under non-strict cutpoint semantics \cite{qiu2009hierarchy, lin2012equivalence}.

\paragraph{Closure Properties}
The set of languages recognized by ML-MMQFA is not closed under union, intersection, or complementation for general acceptance modes. Notably, these properties depend heavily on the acceptance criteria used (bounded-error, cutpoint, etc.) \cite{qiu2009hierarchy}.

\paragraph{Advantages and Limitations}
ML-MMQFA are more powerful than both ML-QFA and MM-1QFA. However, they inherit the undecidability of the emptiness and equivalence problems for non-strict and strict cutpoint semantics \cite{qiu2008decidability, lin2012equivalence}. Their expressive power comes at the cost of analytical and implementation complexity.

\paragraph{Comparison} 
ML-MMQFA strictly subsume ML-QFA under the same input size and acceptance criteria. They are not comparable in power with general QFA models that allow additional memory or two-way movement. Compared to MM-1QFA, ML-MMQFA can accept non-stochastic languages \cite{qiu2009hierarchy}.

\paragraph{Example}
An ML-MMQFA with 2-letter transitions and intermediate measurements can accept the language $(a+b)^*a$ with higher robustness to probabilistic acceptance thresholds than an ML-QFA \cite{belovs2007multi}.

\paragraph{Additional Topics}
Further work focuses on state complexity, succinctness, and simulation algorithms between different classes. The diagonal sum construction plays a key role in analyzing equivalence and decidability \cite{lin2012equivalence}.



\subsubsection{\glsentryfull{ml-revqfa}}
%TODO: https://link.springer.com/chapter/10.1007/978-3-540-73208-2_9 review
\paragraph{Introduction}
\gls{ml-revqfa} extend the multiletter framework by imposing reversibility constraints on the quantum evolution. These automata are designed so that each computation step is invertible, maintaining the core principle of reversibility from quantum mechanics and enhancing coherence preservation \cite{belovs2007multi}.

\paragraph{Formal Definition}
A $k$-letter \gls{ml-revqfa} is a special case of $k$-letter \gls{ml-qfa} where each unitary transformation $\mu'(\omega)$ satisfies the additional constraint that $\mu'(\omega)^{-1} = \mu'(\omega)^\dagger$ for all $\omega \in (\Sigma \cup \{\Lambda\})^k$, and the set of such transformations forms a group under composition.

The automaton is defined as a 5-tuple $A = (Q, Q_{acc}, \ket{\psi_0}, \Sigma, \mu')$, with $\mu'$ restricted to reversible unitary matrices.

\paragraph{Strings Acceptance}
Acceptance is defined as in ML-QFA using a projector $P_{acc}$. For an input string $\omega$, the acceptance probability is:
\[
P_A(\omega) = \| P_{acc} U_{\omega} \ket{\psi_0} \|^2
\]
with $U_{\omega}$ computed from the sequence of reversible unitaries associated with the $k$-letter substrings \cite{belovs2007multi}.

\paragraph{Sets of Languages Accepted}
ML-revQFA are strictly more limited than general ML-QFA due to their reversibility constraint. They can recognize a subset of regular languages and do not accept all regular languages with bounded error, particularly under exact acceptance criteria.

\paragraph{Closure Properties}
The class of languages recognized by ML-revQFA is not closed under union or complement. Reversibility limits the computational power of these models in comparison with more general multiletter QFA models \cite{belovs2007multi}.

\paragraph{Advantages and Limitations}
Reversible automata are appealing for quantum computing implementations due to better coherence and energy efficiency. However, their expressiveness is constrained. ML-revQFA cannot recognize some simple regular languages that non-reversible ML-QFA can handle \cite{belovs2007multi}.

\paragraph{Comparison}
ML-revQFA are less powerful than both ML-QFA and ML-MMQFA. They are closely related to reversible classical automata and group automata. Compared to non-reversible models, they generally require more states or cannot recognize the same languages under equivalent semantics \cite{belovs2007multi}.

\paragraph{Example}
It was shown that ML-revQFA cannot accept the language $(a+b)^*a$ even when using multiletter transitions, while a general ML-QFA can accept it with bounded error \cite{belovs2007multi}.

\paragraph{Additional Topics}
Future research may explore connections with quantum error correction, fault-tolerant reversible computing, and applications in energy-efficient quantum hardware design.


